{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Part I Q1"
      ],
      "metadata": {
        "id": "fjoC-h83Gidh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okKbXogu3bKo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch import nn as nn\n",
        "from torch.nn.functional import normalize\n",
        "from PIL import Image\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train = True, download = True)\n",
        "cifar10_val = datasets.CIFAR10(data_path, train = False, download = True)\n",
        "class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "36a05e32bccd4f2f945c8758474ce1e3",
            "5bd905a1c4244ae08840baa7c9525d75",
            "0b35cefa2eac4d199d5dfe7f481fc971",
            "cfe681db919e4106bb73f83be7a3c0e9",
            "571cc5384e1f4365b072d509ec7e80a7",
            "f1de32fa7c4049d5bfa6ddb550af6438",
            "159f0080c28f415eb458c77a9f16222e",
            "e51a2505a00c49679270d727cb37d306",
            "c30cf19cc0e2459b93e909321370111d",
            "abf14a65ade742f7b058d1144fb6c2f9",
            "dc54f2de664241e3a1966b524ae155a7"
          ]
        },
        "id": "k8seYRA63weO",
        "outputId": "955a8843-e0c8-4397-ac18-1d91b91008e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36a05e32bccd4f2f945c8758474ce1e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(cifar10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGX0b0qU3xRT",
        "outputId": "bb61daf4-40a4-49af-b3db-8c4941ba49e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = cifar10[95]\n",
        "img, label, class_names[label]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC1GXmWV5iZX",
        "outputId": "b8526f4b-b0a4-4d1a-b98c-c345a7ab5ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=RGB size=32x32 at 0x7F530E47D1F0>, 6, 'frog')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "9KOIJwGa6ILM",
        "outputId": "9a1e1652-b5a5-42cb-c404-861e98b2bb42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "9RDWYT8d78Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
        "imgs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAM7BB9s91VZ",
        "outputId": "56302bf8-f913-4496-9724-e3ac41ad13ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32, 50000])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs.view(3, -1).mean(dim = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va7fPstz-KSv",
        "outputId": "bc2175cc-71ae-4865-853c-a9aaa295ced9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4914, 0.4822, 0.4465])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs.view(3, -1).std(dim = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSZ9d1P6-og_",
        "outputId": "62df221d-d8b3-42a1-87e0-c4c8b72b9f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2470, 0.2435, 0.2616])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2470, 0.2435, 0.2616))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SALxYTR__DfY",
        "outputId": "cc259861-b874-4625-f0aa-55d1f055c87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_cifar10 = datasets.CIFAR10(data_path, train = True, download = False, transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2470, 0.2435, 0.2616))]))"
      ],
      "metadata": {
        "id": "IgPBqqgo_4Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(transformed_cifar10, batch_size = 150, shuffle = True)"
      ],
      "metadata": {
        "id": "sLu1HqhRAdG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(3072, 512),\n",
        "                     nn.Tanh(),\n",
        "                     nn.Linear(512, 10),\n",
        "                     nn.LogSoftmax(dim=1))\n",
        "\n",
        "learning_rate = 1e-3\n",
        "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.NLLLoss()\n",
        "n_epochs = 300"
      ],
      "metadata": {
        "id": "hg2q9NrHFt9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tot1 = time.process_time()\n",
        "for epoch in range(1,n_epochs+1):\n",
        "  t1 = time.process_time()\n",
        "  loss_train = 0.0\n",
        "  for imgs, labels in train_loader:\n",
        "    batch_size = imgs.shape[0]\n",
        "    outputs = model(imgs.view(batch_size, -1))\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_train += loss.item()\n",
        "  t2 = time.process_time()\n",
        "\n",
        "  #print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
        "  print(f\"Epoch: %d, Training Loss: %f, Time: {round(t2-t1,4)} seconds\" % (epoch, float(loss)))\n",
        "tot2 = time.process_time()\n",
        "print(f\"Total Time: {round(tot2-tot1,4)} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GOiZ0uFGvhw",
        "outputId": "fbf628a8-1a7f-4ca2-fe45-36c1ea110ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 1.769814, Time: 20.2738 seconds\n",
            "Epoch: 2, Training Loss: 1.765839, Time: 20.2139 seconds\n",
            "Epoch: 3, Training Loss: 1.629620, Time: 20.0953 seconds\n",
            "Epoch: 4, Training Loss: 1.789978, Time: 20.0266 seconds\n",
            "Epoch: 5, Training Loss: 1.660119, Time: 19.9243 seconds\n",
            "Epoch: 6, Training Loss: 1.557614, Time: 19.9228 seconds\n",
            "Epoch: 7, Training Loss: 1.676100, Time: 20.9394 seconds\n",
            "Epoch: 8, Training Loss: 1.712393, Time: 20.4684 seconds\n",
            "Epoch: 9, Training Loss: 1.662656, Time: 19.9176 seconds\n",
            "Epoch: 10, Training Loss: 1.546021, Time: 19.9101 seconds\n",
            "Epoch: 11, Training Loss: 1.584004, Time: 19.9101 seconds\n",
            "Epoch: 12, Training Loss: 1.477909, Time: 19.8376 seconds\n",
            "Epoch: 13, Training Loss: 1.567598, Time: 19.9566 seconds\n",
            "Epoch: 14, Training Loss: 1.487451, Time: 20.0141 seconds\n",
            "Epoch: 15, Training Loss: 1.582643, Time: 20.0614 seconds\n",
            "Epoch: 16, Training Loss: 1.332706, Time: 20.7792 seconds\n",
            "Epoch: 17, Training Loss: 1.450934, Time: 20.014 seconds\n",
            "Epoch: 18, Training Loss: 1.394797, Time: 19.944 seconds\n",
            "Epoch: 19, Training Loss: 1.593890, Time: 19.9648 seconds\n",
            "Epoch: 20, Training Loss: 1.400696, Time: 19.8488 seconds\n",
            "Epoch: 21, Training Loss: 1.435052, Time: 20.2504 seconds\n",
            "Epoch: 22, Training Loss: 1.374351, Time: 20.0134 seconds\n",
            "Epoch: 23, Training Loss: 1.443463, Time: 19.8343 seconds\n",
            "Epoch: 24, Training Loss: 1.404692, Time: 19.8176 seconds\n",
            "Epoch: 25, Training Loss: 1.349757, Time: 20.0159 seconds\n",
            "Epoch: 26, Training Loss: 1.341293, Time: 20.7844 seconds\n",
            "Epoch: 27, Training Loss: 1.325588, Time: 19.8785 seconds\n",
            "Epoch: 28, Training Loss: 1.380654, Time: 19.8689 seconds\n",
            "Epoch: 29, Training Loss: 1.387289, Time: 19.8262 seconds\n",
            "Epoch: 30, Training Loss: 1.325448, Time: 19.7974 seconds\n",
            "Epoch: 31, Training Loss: 1.282982, Time: 19.85 seconds\n",
            "Epoch: 32, Training Loss: 1.326237, Time: 19.9205 seconds\n",
            "Epoch: 33, Training Loss: 1.350094, Time: 20.0605 seconds\n",
            "Epoch: 34, Training Loss: 1.191523, Time: 19.8123 seconds\n",
            "Epoch: 35, Training Loss: 1.198124, Time: 20.6838 seconds\n",
            "Epoch: 36, Training Loss: 1.297725, Time: 19.7932 seconds\n",
            "Epoch: 37, Training Loss: 1.255321, Time: 19.963 seconds\n",
            "Epoch: 38, Training Loss: 1.206745, Time: 19.8468 seconds\n",
            "Epoch: 39, Training Loss: 1.209106, Time: 19.9021 seconds\n",
            "Epoch: 40, Training Loss: 1.169330, Time: 20.0171 seconds\n",
            "Epoch: 41, Training Loss: 1.163223, Time: 20.0608 seconds\n",
            "Epoch: 42, Training Loss: 1.262420, Time: 19.8711 seconds\n",
            "Epoch: 43, Training Loss: 1.129522, Time: 20.329 seconds\n",
            "Epoch: 44, Training Loss: 1.187677, Time: 20.8101 seconds\n",
            "Epoch: 45, Training Loss: 1.128250, Time: 19.8758 seconds\n",
            "Epoch: 46, Training Loss: 1.097416, Time: 19.9768 seconds\n",
            "Epoch: 47, Training Loss: 1.010427, Time: 19.8311 seconds\n",
            "Epoch: 48, Training Loss: 1.107374, Time: 19.8657 seconds\n",
            "Epoch: 49, Training Loss: 1.156967, Time: 19.9112 seconds\n",
            "Epoch: 50, Training Loss: 1.082632, Time: 19.8257 seconds\n",
            "Epoch: 51, Training Loss: 1.065996, Time: 19.8335 seconds\n",
            "Epoch: 52, Training Loss: 0.956227, Time: 19.9078 seconds\n",
            "Epoch: 53, Training Loss: 0.919119, Time: 20.6946 seconds\n",
            "Epoch: 54, Training Loss: 0.967700, Time: 19.9968 seconds\n",
            "Epoch: 55, Training Loss: 0.931164, Time: 19.8177 seconds\n",
            "Epoch: 56, Training Loss: 0.928849, Time: 19.8014 seconds\n",
            "Epoch: 57, Training Loss: 0.879278, Time: 19.945 seconds\n",
            "Epoch: 58, Training Loss: 0.835931, Time: 19.9554 seconds\n",
            "Epoch: 59, Training Loss: 0.986811, Time: 20.1323 seconds\n",
            "Epoch: 60, Training Loss: 0.889088, Time: 19.8395 seconds\n",
            "Epoch: 61, Training Loss: 1.011819, Time: 19.9014 seconds\n",
            "Epoch: 62, Training Loss: 0.886800, Time: 19.8707 seconds\n",
            "Epoch: 63, Training Loss: 0.867117, Time: 20.7481 seconds\n",
            "Epoch: 64, Training Loss: 0.893130, Time: 19.9811 seconds\n",
            "Epoch: 65, Training Loss: 0.894251, Time: 19.8296 seconds\n",
            "Epoch: 66, Training Loss: 0.886598, Time: 19.7818 seconds\n",
            "Epoch: 67, Training Loss: 0.859964, Time: 19.9233 seconds\n",
            "Epoch: 68, Training Loss: 0.758102, Time: 20.3688 seconds\n",
            "Epoch: 69, Training Loss: 0.740202, Time: 19.8717 seconds\n",
            "Epoch: 70, Training Loss: 0.780179, Time: 19.9242 seconds\n",
            "Epoch: 71, Training Loss: 0.731367, Time: 19.875 seconds\n",
            "Epoch: 72, Training Loss: 0.735659, Time: 20.7323 seconds\n",
            "Epoch: 73, Training Loss: 0.793061, Time: 19.8732 seconds\n",
            "Epoch: 74, Training Loss: 0.772290, Time: 19.787 seconds\n",
            "Epoch: 75, Training Loss: 0.717241, Time: 19.8613 seconds\n",
            "Epoch: 76, Training Loss: 0.702831, Time: 19.8782 seconds\n",
            "Epoch: 77, Training Loss: 0.665587, Time: 20.023 seconds\n",
            "Epoch: 78, Training Loss: 0.754575, Time: 19.7828 seconds\n",
            "Epoch: 79, Training Loss: 0.667150, Time: 20.0436 seconds\n",
            "Epoch: 80, Training Loss: 0.768681, Time: 19.887 seconds\n",
            "Epoch: 81, Training Loss: 0.766711, Time: 20.8169 seconds\n",
            "Epoch: 82, Training Loss: 0.731807, Time: 19.9261 seconds\n",
            "Epoch: 83, Training Loss: 0.649006, Time: 19.8299 seconds\n",
            "Epoch: 84, Training Loss: 0.708218, Time: 19.8132 seconds\n",
            "Epoch: 85, Training Loss: 0.722158, Time: 19.9744 seconds\n",
            "Epoch: 86, Training Loss: 0.687097, Time: 19.8318 seconds\n",
            "Epoch: 87, Training Loss: 0.623773, Time: 19.8455 seconds\n",
            "Epoch: 88, Training Loss: 0.697659, Time: 20.4937 seconds\n",
            "Epoch: 89, Training Loss: 0.589095, Time: 19.9265 seconds\n",
            "Epoch: 90, Training Loss: 0.559008, Time: 20.7895 seconds\n",
            "Epoch: 91, Training Loss: 0.566719, Time: 19.9729 seconds\n",
            "Epoch: 92, Training Loss: 0.546324, Time: 19.8504 seconds\n",
            "Epoch: 93, Training Loss: 0.536821, Time: 19.9222 seconds\n",
            "Epoch: 94, Training Loss: 0.524828, Time: 19.8942 seconds\n",
            "Epoch: 95, Training Loss: 0.516155, Time: 19.9861 seconds\n",
            "Epoch: 96, Training Loss: 0.541379, Time: 19.8792 seconds\n",
            "Epoch: 97, Training Loss: 0.505537, Time: 19.9432 seconds\n",
            "Epoch: 98, Training Loss: 0.593904, Time: 19.8482 seconds\n",
            "Epoch: 99, Training Loss: 0.486531, Time: 20.8401 seconds\n",
            "Epoch: 100, Training Loss: 0.569409, Time: 19.9372 seconds\n",
            "Epoch: 101, Training Loss: 0.479202, Time: 19.8603 seconds\n",
            "Epoch: 102, Training Loss: 0.530579, Time: 19.8004 seconds\n",
            "Epoch: 103, Training Loss: 0.485587, Time: 19.8543 seconds\n",
            "Epoch: 104, Training Loss: 0.421336, Time: 19.8034 seconds\n",
            "Epoch: 105, Training Loss: 0.446553, Time: 19.8988 seconds\n",
            "Epoch: 106, Training Loss: 0.466763, Time: 19.8578 seconds\n",
            "Epoch: 107, Training Loss: 0.438357, Time: 19.9091 seconds\n",
            "Epoch: 108, Training Loss: 0.459014, Time: 20.6965 seconds\n",
            "Epoch: 109, Training Loss: 0.488158, Time: 19.8279 seconds\n",
            "Epoch: 110, Training Loss: 0.454764, Time: 19.9353 seconds\n",
            "Epoch: 111, Training Loss: 0.432774, Time: 19.8134 seconds\n",
            "Epoch: 112, Training Loss: 0.391203, Time: 19.8814 seconds\n",
            "Epoch: 113, Training Loss: 0.423508, Time: 19.9392 seconds\n",
            "Epoch: 114, Training Loss: 0.425718, Time: 19.8356 seconds\n",
            "Epoch: 115, Training Loss: 0.405338, Time: 19.8266 seconds\n",
            "Epoch: 116, Training Loss: 0.490088, Time: 19.802 seconds\n",
            "Epoch: 117, Training Loss: 0.362152, Time: 19.9303 seconds\n",
            "Epoch: 118, Training Loss: 0.369883, Time: 20.7505 seconds\n",
            "Epoch: 119, Training Loss: 0.425820, Time: 19.8465 seconds\n",
            "Epoch: 120, Training Loss: 0.315156, Time: 19.8409 seconds\n",
            "Epoch: 121, Training Loss: 0.362412, Time: 19.8766 seconds\n",
            "Epoch: 122, Training Loss: 0.362313, Time: 19.8341 seconds\n",
            "Epoch: 123, Training Loss: 0.415816, Time: 19.8365 seconds\n",
            "Epoch: 124, Training Loss: 0.342536, Time: 19.7724 seconds\n",
            "Epoch: 125, Training Loss: 0.356210, Time: 19.7766 seconds\n",
            "Epoch: 126, Training Loss: 0.368818, Time: 19.8518 seconds\n",
            "Epoch: 127, Training Loss: 0.388856, Time: 20.6885 seconds\n",
            "Epoch: 128, Training Loss: 0.314638, Time: 19.8706 seconds\n",
            "Epoch: 129, Training Loss: 0.355842, Time: 20.0193 seconds\n",
            "Epoch: 130, Training Loss: 0.309614, Time: 19.8955 seconds\n",
            "Epoch: 131, Training Loss: 0.340514, Time: 19.8552 seconds\n",
            "Epoch: 132, Training Loss: 0.384978, Time: 19.8118 seconds\n",
            "Epoch: 133, Training Loss: 0.341783, Time: 19.7444 seconds\n",
            "Epoch: 134, Training Loss: 0.334384, Time: 19.8022 seconds\n",
            "Epoch: 135, Training Loss: 0.290419, Time: 19.8424 seconds\n",
            "Epoch: 136, Training Loss: 0.346544, Time: 19.922 seconds\n",
            "Epoch: 137, Training Loss: 0.285178, Time: 20.846 seconds\n",
            "Epoch: 138, Training Loss: 0.308468, Time: 19.9167 seconds\n",
            "Epoch: 139, Training Loss: 0.264685, Time: 19.9348 seconds\n",
            "Epoch: 140, Training Loss: 0.318358, Time: 19.779 seconds\n",
            "Epoch: 141, Training Loss: 0.272249, Time: 19.895 seconds\n",
            "Epoch: 142, Training Loss: 0.266361, Time: 19.9381 seconds\n",
            "Epoch: 143, Training Loss: 0.257084, Time: 19.8626 seconds\n",
            "Epoch: 144, Training Loss: 0.256672, Time: 19.7674 seconds\n",
            "Epoch: 145, Training Loss: 0.229276, Time: 19.7753 seconds\n",
            "Epoch: 146, Training Loss: 0.210531, Time: 20.7531 seconds\n",
            "Epoch: 147, Training Loss: 0.208676, Time: 19.8486 seconds\n",
            "Epoch: 148, Training Loss: 0.265893, Time: 19.7458 seconds\n",
            "Epoch: 149, Training Loss: 0.195313, Time: 19.9592 seconds\n",
            "Epoch: 150, Training Loss: 0.265482, Time: 19.7404 seconds\n",
            "Epoch: 151, Training Loss: 0.235865, Time: 19.8104 seconds\n",
            "Epoch: 152, Training Loss: 0.220729, Time: 19.798 seconds\n",
            "Epoch: 153, Training Loss: 0.205575, Time: 19.8386 seconds\n",
            "Epoch: 154, Training Loss: 0.250392, Time: 19.8289 seconds\n",
            "Epoch: 155, Training Loss: 0.202377, Time: 19.8044 seconds\n",
            "Epoch: 156, Training Loss: 0.193996, Time: 20.6189 seconds\n",
            "Epoch: 157, Training Loss: 0.211604, Time: 19.9671 seconds\n",
            "Epoch: 158, Training Loss: 0.189501, Time: 19.9135 seconds\n",
            "Epoch: 159, Training Loss: 0.207688, Time: 19.8224 seconds\n",
            "Epoch: 160, Training Loss: 0.175570, Time: 19.8303 seconds\n",
            "Epoch: 161, Training Loss: 0.177055, Time: 19.7445 seconds\n",
            "Epoch: 162, Training Loss: 0.203022, Time: 19.9797 seconds\n",
            "Epoch: 163, Training Loss: 0.174923, Time: 19.7081 seconds\n",
            "Epoch: 164, Training Loss: 0.192598, Time: 19.7626 seconds\n",
            "Epoch: 165, Training Loss: 0.154607, Time: 20.6751 seconds\n",
            "Epoch: 166, Training Loss: 0.203393, Time: 19.852 seconds\n",
            "Epoch: 167, Training Loss: 0.198512, Time: 19.7304 seconds\n",
            "Epoch: 168, Training Loss: 0.171918, Time: 19.7261 seconds\n",
            "Epoch: 169, Training Loss: 0.172865, Time: 19.8194 seconds\n",
            "Epoch: 170, Training Loss: 0.143902, Time: 19.8129 seconds\n",
            "Epoch: 171, Training Loss: 0.197276, Time: 19.802 seconds\n",
            "Epoch: 172, Training Loss: 0.170945, Time: 19.8387 seconds\n",
            "Epoch: 173, Training Loss: 0.163511, Time: 19.7913 seconds\n",
            "Epoch: 174, Training Loss: 0.151378, Time: 20.7924 seconds\n",
            "Epoch: 175, Training Loss: 0.142370, Time: 19.8572 seconds\n",
            "Epoch: 176, Training Loss: 0.127701, Time: 19.8283 seconds\n",
            "Epoch: 177, Training Loss: 0.139256, Time: 19.8002 seconds\n",
            "Epoch: 178, Training Loss: 0.146795, Time: 19.8829 seconds\n",
            "Epoch: 179, Training Loss: 0.140617, Time: 19.773 seconds\n",
            "Epoch: 180, Training Loss: 0.122482, Time: 19.7942 seconds\n",
            "Epoch: 181, Training Loss: 0.164297, Time: 19.7929 seconds\n",
            "Epoch: 182, Training Loss: 0.162827, Time: 19.8059 seconds\n",
            "Epoch: 183, Training Loss: 0.139271, Time: 19.8057 seconds\n",
            "Epoch: 184, Training Loss: 0.115270, Time: 20.6867 seconds\n",
            "Epoch: 185, Training Loss: 0.102828, Time: 19.8605 seconds\n",
            "Epoch: 186, Training Loss: 0.143872, Time: 19.7033 seconds\n",
            "Epoch: 187, Training Loss: 0.145501, Time: 19.7712 seconds\n",
            "Epoch: 188, Training Loss: 0.150504, Time: 19.851 seconds\n",
            "Epoch: 189, Training Loss: 0.124758, Time: 19.8117 seconds\n",
            "Epoch: 190, Training Loss: 0.126680, Time: 19.8432 seconds\n",
            "Epoch: 191, Training Loss: 0.109971, Time: 19.7645 seconds\n",
            "Epoch: 192, Training Loss: 0.113889, Time: 19.7392 seconds\n",
            "Epoch: 193, Training Loss: 0.129989, Time: 20.717 seconds\n",
            "Epoch: 194, Training Loss: 0.127900, Time: 19.8466 seconds\n",
            "Epoch: 195, Training Loss: 0.103351, Time: 19.7985 seconds\n",
            "Epoch: 196, Training Loss: 0.118262, Time: 19.8787 seconds\n",
            "Epoch: 197, Training Loss: 0.111592, Time: 19.8019 seconds\n",
            "Epoch: 198, Training Loss: 0.122063, Time: 19.8744 seconds\n",
            "Epoch: 199, Training Loss: 0.088311, Time: 19.9007 seconds\n",
            "Epoch: 200, Training Loss: 0.106607, Time: 19.8529 seconds\n",
            "Epoch: 201, Training Loss: 0.106045, Time: 19.8335 seconds\n",
            "Epoch: 202, Training Loss: 0.095618, Time: 19.8116 seconds\n",
            "Epoch: 203, Training Loss: 0.097493, Time: 20.7562 seconds\n",
            "Epoch: 204, Training Loss: 0.105270, Time: 19.9358 seconds\n",
            "Epoch: 205, Training Loss: 0.092254, Time: 19.8483 seconds\n",
            "Epoch: 206, Training Loss: 0.092555, Time: 19.8401 seconds\n",
            "Epoch: 207, Training Loss: 0.092152, Time: 19.8339 seconds\n",
            "Epoch: 208, Training Loss: 0.106246, Time: 19.8742 seconds\n",
            "Epoch: 209, Training Loss: 0.098420, Time: 19.7505 seconds\n",
            "Epoch: 210, Training Loss: 0.127683, Time: 19.7553 seconds\n",
            "Epoch: 211, Training Loss: 0.097329, Time: 19.8044 seconds\n",
            "Epoch: 212, Training Loss: 0.085888, Time: 20.5761 seconds\n",
            "Epoch: 213, Training Loss: 0.093054, Time: 19.7293 seconds\n",
            "Epoch: 214, Training Loss: 0.130508, Time: 19.9237 seconds\n",
            "Epoch: 215, Training Loss: 0.093524, Time: 19.7953 seconds\n",
            "Epoch: 216, Training Loss: 0.081684, Time: 19.7878 seconds\n",
            "Epoch: 217, Training Loss: 0.084037, Time: 19.7904 seconds\n",
            "Epoch: 218, Training Loss: 0.100016, Time: 19.8937 seconds\n",
            "Epoch: 219, Training Loss: 0.095138, Time: 19.868 seconds\n",
            "Epoch: 220, Training Loss: 0.083469, Time: 19.8321 seconds\n",
            "Epoch: 221, Training Loss: 0.089287, Time: 19.8528 seconds\n",
            "Epoch: 222, Training Loss: 0.085387, Time: 20.633 seconds\n",
            "Epoch: 223, Training Loss: 0.071861, Time: 19.8053 seconds\n",
            "Epoch: 224, Training Loss: 0.076970, Time: 19.7465 seconds\n",
            "Epoch: 225, Training Loss: 0.071857, Time: 19.7788 seconds\n",
            "Epoch: 226, Training Loss: 0.083045, Time: 19.8578 seconds\n",
            "Epoch: 227, Training Loss: 0.086828, Time: 19.8157 seconds\n",
            "Epoch: 228, Training Loss: 0.076681, Time: 19.7601 seconds\n",
            "Epoch: 229, Training Loss: 0.065781, Time: 19.8546 seconds\n",
            "Epoch: 230, Training Loss: 0.080688, Time: 19.7686 seconds\n",
            "Epoch: 231, Training Loss: 0.069915, Time: 20.4586 seconds\n",
            "Epoch: 232, Training Loss: 0.066602, Time: 20.1999 seconds\n",
            "Epoch: 233, Training Loss: 0.073323, Time: 19.8459 seconds\n",
            "Epoch: 234, Training Loss: 0.073741, Time: 19.8137 seconds\n",
            "Epoch: 235, Training Loss: 0.070592, Time: 19.9475 seconds\n",
            "Epoch: 236, Training Loss: 0.086146, Time: 19.8657 seconds\n",
            "Epoch: 237, Training Loss: 0.075804, Time: 19.9522 seconds\n",
            "Epoch: 238, Training Loss: 0.084149, Time: 19.8661 seconds\n",
            "Epoch: 239, Training Loss: 0.070928, Time: 19.8097 seconds\n",
            "Epoch: 240, Training Loss: 0.058468, Time: 19.8136 seconds\n",
            "Epoch: 241, Training Loss: 0.067453, Time: 20.7152 seconds\n",
            "Epoch: 242, Training Loss: 0.064168, Time: 19.8704 seconds\n",
            "Epoch: 243, Training Loss: 0.069233, Time: 19.8851 seconds\n",
            "Epoch: 244, Training Loss: 0.068109, Time: 19.9117 seconds\n",
            "Epoch: 245, Training Loss: 0.071608, Time: 19.8569 seconds\n",
            "Epoch: 246, Training Loss: 0.080528, Time: 19.7961 seconds\n",
            "Epoch: 247, Training Loss: 0.067455, Time: 19.8965 seconds\n",
            "Epoch: 248, Training Loss: 0.068606, Time: 19.8573 seconds\n",
            "Epoch: 249, Training Loss: 0.056246, Time: 20.1395 seconds\n",
            "Epoch: 250, Training Loss: 0.072297, Time: 20.4175 seconds\n",
            "Epoch: 251, Training Loss: 0.064293, Time: 20.2398 seconds\n",
            "Epoch: 252, Training Loss: 0.057698, Time: 19.8058 seconds\n",
            "Epoch: 253, Training Loss: 0.057653, Time: 19.8214 seconds\n",
            "Epoch: 254, Training Loss: 0.059430, Time: 19.7818 seconds\n",
            "Epoch: 255, Training Loss: 0.051105, Time: 19.9231 seconds\n",
            "Epoch: 256, Training Loss: 0.061653, Time: 19.9607 seconds\n",
            "Epoch: 257, Training Loss: 0.072628, Time: 19.8191 seconds\n",
            "Epoch: 258, Training Loss: 0.054484, Time: 19.8204 seconds\n",
            "Epoch: 259, Training Loss: 0.055097, Time: 19.8404 seconds\n",
            "Epoch: 260, Training Loss: 0.064547, Time: 20.6707 seconds\n",
            "Epoch: 261, Training Loss: 0.061564, Time: 19.8176 seconds\n",
            "Epoch: 262, Training Loss: 0.054252, Time: 19.8535 seconds\n",
            "Epoch: 263, Training Loss: 0.059177, Time: 19.8345 seconds\n",
            "Epoch: 264, Training Loss: 0.054061, Time: 19.8365 seconds\n",
            "Epoch: 265, Training Loss: 0.051810, Time: 19.7483 seconds\n",
            "Epoch: 266, Training Loss: 0.059992, Time: 19.8893 seconds\n",
            "Epoch: 267, Training Loss: 0.048877, Time: 19.8643 seconds\n",
            "Epoch: 268, Training Loss: 0.058204, Time: 19.8868 seconds\n",
            "Epoch: 269, Training Loss: 0.044242, Time: 20.726 seconds\n",
            "Epoch: 270, Training Loss: 0.054433, Time: 19.8463 seconds\n",
            "Epoch: 271, Training Loss: 0.043792, Time: 19.8537 seconds\n",
            "Epoch: 272, Training Loss: 0.061280, Time: 19.9315 seconds\n",
            "Epoch: 273, Training Loss: 0.034547, Time: 19.8348 seconds\n",
            "Epoch: 274, Training Loss: 0.052653, Time: 19.9017 seconds\n",
            "Epoch: 275, Training Loss: 0.044022, Time: 19.8912 seconds\n",
            "Epoch: 276, Training Loss: 0.049402, Time: 19.8724 seconds\n",
            "Epoch: 277, Training Loss: 0.051532, Time: 19.7531 seconds\n",
            "Epoch: 278, Training Loss: 0.039112, Time: 19.8106 seconds\n",
            "Epoch: 279, Training Loss: 0.042926, Time: 20.6867 seconds\n",
            "Epoch: 280, Training Loss: 0.047167, Time: 19.9069 seconds\n",
            "Epoch: 281, Training Loss: 0.039085, Time: 19.8381 seconds\n",
            "Epoch: 282, Training Loss: 0.038206, Time: 19.9506 seconds\n",
            "Epoch: 283, Training Loss: 0.044637, Time: 19.7124 seconds\n",
            "Epoch: 284, Training Loss: 0.047889, Time: 19.8403 seconds\n",
            "Epoch: 285, Training Loss: 0.057486, Time: 20.1393 seconds\n",
            "Epoch: 286, Training Loss: 0.051243, Time: 19.8444 seconds\n",
            "Epoch: 287, Training Loss: 0.046536, Time: 19.8676 seconds\n",
            "Epoch: 288, Training Loss: 0.042986, Time: 20.777 seconds\n",
            "Epoch: 289, Training Loss: 0.038362, Time: 19.9002 seconds\n",
            "Epoch: 290, Training Loss: 0.044302, Time: 19.8302 seconds\n",
            "Epoch: 291, Training Loss: 0.046386, Time: 19.7882 seconds\n",
            "Epoch: 292, Training Loss: 0.045329, Time: 19.7976 seconds\n",
            "Epoch: 293, Training Loss: 0.040177, Time: 19.8045 seconds\n",
            "Epoch: 294, Training Loss: 0.043560, Time: 19.8308 seconds\n",
            "Epoch: 295, Training Loss: 0.046792, Time: 19.7866 seconds\n",
            "Epoch: 296, Training Loss: 0.044016, Time: 19.8295 seconds\n",
            "Epoch: 297, Training Loss: 0.047018, Time: 19.9597 seconds\n",
            "Epoch: 298, Training Loss: 0.039908, Time: 20.8503 seconds\n",
            "Epoch: 299, Training Loss: 0.044229, Time: 19.8866 seconds\n",
            "Epoch: 300, Training Loss: 0.046498, Time: 19.7643 seconds\n",
            "Total Time: 5990.5077 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_cifar10_val = datasets.CIFAR10(data_path, train = False, download = False, transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2470, 0.2435, 0.2616))]))\n",
        "val_loader = torch.utils.data.DataLoader(transformed_cifar10_val, batch_size = 150, shuffle = False)"
      ],
      "metadata": {
        "id": "gPQEfMzZ0waa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "t3 = time.process_time()\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        batch_size = imgs.shape[0]\n",
        "        outputs = model(imgs.view(batch_size, -1))\n",
        "        _, predicted = torch.max(outputs, dim = 1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "t4= time.process_time()\n",
        "print(f\"Accuracy: {round(correct/total, 3)}, Time: {round(t4 - t3, 3)} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK9abZaFHqIF",
        "outputId": "f4f50781-afd7-45c8-d3e6-9884a063f45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.469, Time: 3.279 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part I Q2"
      ],
      "metadata": {
        "id": "TxTrzChqGc1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = nn.Sequential(nn.Linear(3072, 512),\n",
        "                     nn.Tanh(),\n",
        "                     nn.Linear(512, 128),\n",
        "                     nn.Tanh(),\n",
        "                     nn.Linear(128, 64),\n",
        "                     nn.Tanh(),\n",
        "                     nn.Linear(64, 16),\n",
        "                     nn.LogSoftmax(dim=1))\n",
        "\n",
        "optimizer2 = optim.SGD(model2.parameters(), lr = learning_rate)\n",
        "loss_fn2 = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "siMMfe-Ibing"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tot3 = time.process_time()\n",
        "for epoch in range(1,n_epochs+1):\n",
        "  t5 = time.process_time()\n",
        "  loss_train = 0.0\n",
        "  for imgs, labels in train_loader:\n",
        "    batch_size = imgs.shape[0]\n",
        "    outputs = model2(imgs.view(batch_size, -1))\n",
        "    loss = loss_fn2(outputs, labels)\n",
        "\n",
        "    optimizer2.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer2.step()\n",
        "\n",
        "    loss_train += loss.item()\n",
        "  t6 = time.process_time()\n",
        "\n",
        "  #print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
        "  print(f\"Epoch: %d, Training Loss: %f, Time: {round(t6-t5,4)} seconds\" % (epoch, float(loss)))\n",
        "tot4 = time.process_time()\n",
        "print(f\"Total Time: {round(tot4-tot3,4)} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM3ecIfff2i0",
        "outputId": "40b369d0-dd1a-4add-db9e-bda3711262dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 2.195786, Time: 21.9852 seconds\n",
            "Epoch: 2, Training Loss: 2.101866, Time: 21.3548 seconds\n",
            "Epoch: 3, Training Loss: 1.918546, Time: 21.5113 seconds\n",
            "Epoch: 4, Training Loss: 1.923468, Time: 21.4547 seconds\n",
            "Epoch: 5, Training Loss: 1.932271, Time: 21.5727 seconds\n",
            "Epoch: 6, Training Loss: 1.718220, Time: 21.6506 seconds\n",
            "Epoch: 7, Training Loss: 1.727247, Time: 21.3284 seconds\n",
            "Epoch: 8, Training Loss: 1.763041, Time: 20.8493 seconds\n",
            "Epoch: 9, Training Loss: 1.769154, Time: 20.9327 seconds\n",
            "Epoch: 10, Training Loss: 1.667553, Time: 21.8751 seconds\n",
            "Epoch: 11, Training Loss: 1.639545, Time: 20.9658 seconds\n",
            "Epoch: 12, Training Loss: 1.600632, Time: 20.9499 seconds\n",
            "Epoch: 13, Training Loss: 1.505273, Time: 21.2031 seconds\n",
            "Epoch: 14, Training Loss: 1.613857, Time: 21.0364 seconds\n",
            "Epoch: 15, Training Loss: 1.555218, Time: 21.1158 seconds\n",
            "Epoch: 16, Training Loss: 1.586445, Time: 20.5558 seconds\n",
            "Epoch: 17, Training Loss: 1.570115, Time: 20.7841 seconds\n",
            "Epoch: 18, Training Loss: 1.435164, Time: 21.0597 seconds\n",
            "Epoch: 19, Training Loss: 1.594139, Time: 21.5486 seconds\n",
            "Epoch: 20, Training Loss: 1.440851, Time: 20.6 seconds\n",
            "Epoch: 21, Training Loss: 1.394163, Time: 21.282 seconds\n",
            "Epoch: 22, Training Loss: 1.466990, Time: 21.3977 seconds\n",
            "Epoch: 23, Training Loss: 1.405260, Time: 21.1934 seconds\n",
            "Epoch: 24, Training Loss: 1.266999, Time: 21.0201 seconds\n",
            "Epoch: 25, Training Loss: 1.388929, Time: 21.0717 seconds\n",
            "Epoch: 26, Training Loss: 1.417435, Time: 21.49 seconds\n",
            "Epoch: 27, Training Loss: 1.370648, Time: 20.6507 seconds\n",
            "Epoch: 28, Training Loss: 1.292662, Time: 22.1586 seconds\n",
            "Epoch: 29, Training Loss: 1.503182, Time: 21.0568 seconds\n",
            "Epoch: 30, Training Loss: 1.240309, Time: 21.2522 seconds\n",
            "Epoch: 31, Training Loss: 1.307980, Time: 21.2765 seconds\n",
            "Epoch: 32, Training Loss: 1.424298, Time: 21.4368 seconds\n",
            "Epoch: 33, Training Loss: 1.244659, Time: 21.144 seconds\n",
            "Epoch: 34, Training Loss: 1.281661, Time: 20.7925 seconds\n",
            "Epoch: 35, Training Loss: 1.303211, Time: 20.9165 seconds\n",
            "Epoch: 36, Training Loss: 1.204792, Time: 21.0303 seconds\n",
            "Epoch: 37, Training Loss: 1.172885, Time: 21.6727 seconds\n",
            "Epoch: 38, Training Loss: 1.082009, Time: 21.0085 seconds\n",
            "Epoch: 39, Training Loss: 1.019789, Time: 21.0718 seconds\n",
            "Epoch: 40, Training Loss: 1.224284, Time: 20.6553 seconds\n",
            "Epoch: 41, Training Loss: 1.060049, Time: 21.3287 seconds\n",
            "Epoch: 42, Training Loss: 1.097918, Time: 20.599 seconds\n",
            "Epoch: 43, Training Loss: 1.021692, Time: 20.9471 seconds\n",
            "Epoch: 44, Training Loss: 0.942073, Time: 20.5888 seconds\n",
            "Epoch: 45, Training Loss: 0.982646, Time: 21.8224 seconds\n",
            "Epoch: 46, Training Loss: 0.979219, Time: 21.0108 seconds\n",
            "Epoch: 47, Training Loss: 0.912297, Time: 20.9196 seconds\n",
            "Epoch: 48, Training Loss: 1.030454, Time: 20.4279 seconds\n",
            "Epoch: 49, Training Loss: 0.939958, Time: 20.7713 seconds\n",
            "Epoch: 50, Training Loss: 0.907642, Time: 21.4651 seconds\n",
            "Epoch: 51, Training Loss: 0.958434, Time: 21.1249 seconds\n",
            "Epoch: 52, Training Loss: 0.989461, Time: 21.1129 seconds\n",
            "Epoch: 53, Training Loss: 0.846964, Time: 20.8241 seconds\n",
            "Epoch: 54, Training Loss: 0.723026, Time: 21.8106 seconds\n",
            "Epoch: 55, Training Loss: 0.808746, Time: 21.164 seconds\n",
            "Epoch: 56, Training Loss: 0.776948, Time: 21.1268 seconds\n",
            "Epoch: 57, Training Loss: 0.880001, Time: 20.9045 seconds\n",
            "Epoch: 58, Training Loss: 0.836816, Time: 21.3044 seconds\n",
            "Epoch: 59, Training Loss: 0.873330, Time: 21.3092 seconds\n",
            "Epoch: 60, Training Loss: 0.753591, Time: 21.4797 seconds\n",
            "Epoch: 61, Training Loss: 0.697301, Time: 21.4904 seconds\n",
            "Epoch: 62, Training Loss: 0.747466, Time: 21.5299 seconds\n",
            "Epoch: 63, Training Loss: 0.588338, Time: 22.5483 seconds\n",
            "Epoch: 64, Training Loss: 0.652785, Time: 21.6351 seconds\n",
            "Epoch: 65, Training Loss: 0.621209, Time: 21.2772 seconds\n",
            "Epoch: 66, Training Loss: 0.813992, Time: 20.8726 seconds\n",
            "Epoch: 67, Training Loss: 0.649181, Time: 20.675 seconds\n",
            "Epoch: 68, Training Loss: 0.620332, Time: 20.6421 seconds\n",
            "Epoch: 69, Training Loss: 0.629729, Time: 21.1721 seconds\n",
            "Epoch: 70, Training Loss: 0.780360, Time: 21.2358 seconds\n",
            "Epoch: 71, Training Loss: 0.590880, Time: 20.8673 seconds\n",
            "Epoch: 72, Training Loss: 0.495676, Time: 21.7661 seconds\n",
            "Epoch: 73, Training Loss: 0.570425, Time: 20.6556 seconds\n",
            "Epoch: 74, Training Loss: 0.393251, Time: 21.0148 seconds\n",
            "Epoch: 75, Training Loss: 0.567395, Time: 20.6434 seconds\n",
            "Epoch: 76, Training Loss: 0.580145, Time: 20.2382 seconds\n",
            "Epoch: 77, Training Loss: 0.474137, Time: 20.7256 seconds\n",
            "Epoch: 78, Training Loss: 0.378497, Time: 20.2338 seconds\n",
            "Epoch: 79, Training Loss: 0.573543, Time: 20.3343 seconds\n",
            "Epoch: 80, Training Loss: 0.463623, Time: 20.5107 seconds\n",
            "Epoch: 81, Training Loss: 0.544208, Time: 21.3102 seconds\n",
            "Epoch: 82, Training Loss: 0.472446, Time: 21.07 seconds\n",
            "Epoch: 83, Training Loss: 0.330699, Time: 20.8982 seconds\n",
            "Epoch: 84, Training Loss: 0.357056, Time: 21.064 seconds\n",
            "Epoch: 85, Training Loss: 0.403832, Time: 20.8397 seconds\n",
            "Epoch: 86, Training Loss: 0.353314, Time: 21.3985 seconds\n",
            "Epoch: 87, Training Loss: 0.247991, Time: 20.8115 seconds\n",
            "Epoch: 88, Training Loss: 0.636982, Time: 20.7322 seconds\n",
            "Epoch: 89, Training Loss: 0.240460, Time: 20.513 seconds\n",
            "Epoch: 90, Training Loss: 0.260562, Time: 21.7403 seconds\n",
            "Epoch: 91, Training Loss: 0.337354, Time: 20.1018 seconds\n",
            "Epoch: 92, Training Loss: 0.202390, Time: 21.2841 seconds\n",
            "Epoch: 93, Training Loss: 0.259856, Time: 21.1528 seconds\n",
            "Epoch: 94, Training Loss: 0.192237, Time: 20.8287 seconds\n",
            "Epoch: 95, Training Loss: 0.294651, Time: 20.9161 seconds\n",
            "Epoch: 96, Training Loss: 0.251418, Time: 20.8303 seconds\n",
            "Epoch: 97, Training Loss: 0.162145, Time: 21.2524 seconds\n",
            "Epoch: 98, Training Loss: 0.289336, Time: 21.5302 seconds\n",
            "Epoch: 99, Training Loss: 0.204323, Time: 21.6533 seconds\n",
            "Epoch: 100, Training Loss: 0.195064, Time: 21.3369 seconds\n",
            "Epoch: 101, Training Loss: 0.194739, Time: 20.9958 seconds\n",
            "Epoch: 102, Training Loss: 0.253700, Time: 21.0319 seconds\n",
            "Epoch: 103, Training Loss: 0.138699, Time: 21.302 seconds\n",
            "Epoch: 104, Training Loss: 0.174339, Time: 20.7338 seconds\n",
            "Epoch: 105, Training Loss: 0.148126, Time: 20.7651 seconds\n",
            "Epoch: 106, Training Loss: 0.205731, Time: 20.6924 seconds\n",
            "Epoch: 107, Training Loss: 0.113202, Time: 20.7287 seconds\n",
            "Epoch: 108, Training Loss: 0.127745, Time: 21.6718 seconds\n",
            "Epoch: 109, Training Loss: 0.124082, Time: 20.6671 seconds\n",
            "Epoch: 110, Training Loss: 0.126115, Time: 21.2494 seconds\n",
            "Epoch: 111, Training Loss: 0.275492, Time: 21.4147 seconds\n",
            "Epoch: 112, Training Loss: 0.067170, Time: 21.6158 seconds\n",
            "Epoch: 113, Training Loss: 0.100481, Time: 21.26 seconds\n",
            "Epoch: 114, Training Loss: 0.084464, Time: 20.9354 seconds\n",
            "Epoch: 115, Training Loss: 0.117644, Time: 21.0084 seconds\n",
            "Epoch: 116, Training Loss: 0.105209, Time: 21.4272 seconds\n",
            "Epoch: 117, Training Loss: 0.061587, Time: 22.1775 seconds\n",
            "Epoch: 118, Training Loss: 0.075833, Time: 21.2317 seconds\n",
            "Epoch: 119, Training Loss: 0.070217, Time: 21.3285 seconds\n",
            "Epoch: 120, Training Loss: 0.089775, Time: 21.401 seconds\n",
            "Epoch: 121, Training Loss: 0.069761, Time: 21.6124 seconds\n",
            "Epoch: 122, Training Loss: 0.073051, Time: 20.5223 seconds\n",
            "Epoch: 123, Training Loss: 0.080512, Time: 21.8864 seconds\n",
            "Epoch: 124, Training Loss: 0.093527, Time: 21.3198 seconds\n",
            "Epoch: 125, Training Loss: 1.094125, Time: 21.0431 seconds\n",
            "Epoch: 126, Training Loss: 0.060904, Time: 22.0976 seconds\n",
            "Epoch: 127, Training Loss: 0.039988, Time: 20.7583 seconds\n",
            "Epoch: 128, Training Loss: 0.036975, Time: 21.3545 seconds\n",
            "Epoch: 129, Training Loss: 0.574920, Time: 21.0712 seconds\n",
            "Epoch: 130, Training Loss: 0.053937, Time: 20.7213 seconds\n",
            "Epoch: 131, Training Loss: 0.035462, Time: 21.5237 seconds\n",
            "Epoch: 132, Training Loss: 0.105386, Time: 21.2404 seconds\n",
            "Epoch: 133, Training Loss: 0.046789, Time: 21.6561 seconds\n",
            "Epoch: 134, Training Loss: 0.027788, Time: 22.0046 seconds\n",
            "Epoch: 135, Training Loss: 0.049616, Time: 22.1944 seconds\n",
            "Epoch: 136, Training Loss: 0.030450, Time: 21.3006 seconds\n",
            "Epoch: 137, Training Loss: 0.024809, Time: 21.3527 seconds\n",
            "Epoch: 138, Training Loss: 0.162401, Time: 21.5286 seconds\n",
            "Epoch: 139, Training Loss: 0.051671, Time: 21.2099 seconds\n",
            "Epoch: 140, Training Loss: 0.024773, Time: 20.6344 seconds\n",
            "Epoch: 141, Training Loss: 0.027329, Time: 21.3016 seconds\n",
            "Epoch: 142, Training Loss: 0.020197, Time: 21.0993 seconds\n",
            "Epoch: 143, Training Loss: 0.033703, Time: 21.4184 seconds\n",
            "Epoch: 144, Training Loss: 0.061639, Time: 22.6888 seconds\n",
            "Epoch: 145, Training Loss: 0.026729, Time: 21.7411 seconds\n",
            "Epoch: 146, Training Loss: 0.022500, Time: 21.3857 seconds\n",
            "Epoch: 147, Training Loss: 0.022522, Time: 20.8406 seconds\n",
            "Epoch: 148, Training Loss: 0.017197, Time: 21.2131 seconds\n",
            "Epoch: 149, Training Loss: 0.017585, Time: 20.8943 seconds\n",
            "Epoch: 150, Training Loss: 0.018658, Time: 20.2937 seconds\n",
            "Epoch: 151, Training Loss: 0.016161, Time: 21.4182 seconds\n",
            "Epoch: 152, Training Loss: 0.015553, Time: 20.1924 seconds\n",
            "Epoch: 153, Training Loss: 1.158137, Time: 21.9562 seconds\n",
            "Epoch: 154, Training Loss: 0.025367, Time: 20.8082 seconds\n",
            "Epoch: 155, Training Loss: 0.020978, Time: 21.0431 seconds\n",
            "Epoch: 156, Training Loss: 0.015716, Time: 20.5198 seconds\n",
            "Epoch: 157, Training Loss: 0.017525, Time: 21.0216 seconds\n",
            "Epoch: 158, Training Loss: 0.014540, Time: 20.6248 seconds\n",
            "Epoch: 159, Training Loss: 0.015628, Time: 21.0179 seconds\n",
            "Epoch: 160, Training Loss: 0.011615, Time: 21.0852 seconds\n",
            "Epoch: 161, Training Loss: 0.014051, Time: 21.407 seconds\n",
            "Epoch: 162, Training Loss: 0.010233, Time: 22.2344 seconds\n",
            "Epoch: 163, Training Loss: 0.010655, Time: 21.2282 seconds\n",
            "Epoch: 164, Training Loss: 0.014912, Time: 21.4224 seconds\n",
            "Epoch: 165, Training Loss: 0.010264, Time: 20.8706 seconds\n",
            "Epoch: 166, Training Loss: 0.011869, Time: 20.838 seconds\n",
            "Epoch: 167, Training Loss: 0.011472, Time: 20.7615 seconds\n",
            "Epoch: 168, Training Loss: 0.010979, Time: 21.0976 seconds\n",
            "Epoch: 169, Training Loss: 0.009787, Time: 21.3039 seconds\n",
            "Epoch: 170, Training Loss: 0.009382, Time: 21.5542 seconds\n",
            "Epoch: 171, Training Loss: 0.009551, Time: 22.3961 seconds\n",
            "Epoch: 172, Training Loss: 0.012068, Time: 21.0345 seconds\n",
            "Epoch: 173, Training Loss: 0.008339, Time: 21.174 seconds\n",
            "Epoch: 174, Training Loss: 0.008546, Time: 20.8641 seconds\n",
            "Epoch: 175, Training Loss: 0.010625, Time: 21.144 seconds\n",
            "Epoch: 176, Training Loss: 0.007715, Time: 21.315 seconds\n",
            "Epoch: 177, Training Loss: 0.008270, Time: 21.1343 seconds\n",
            "Epoch: 178, Training Loss: 0.010745, Time: 20.8118 seconds\n",
            "Epoch: 179, Training Loss: 0.009029, Time: 21.0339 seconds\n",
            "Epoch: 180, Training Loss: 0.007894, Time: 21.5963 seconds\n",
            "Epoch: 181, Training Loss: 0.008744, Time: 20.8338 seconds\n",
            "Epoch: 182, Training Loss: 0.008534, Time: 21.2211 seconds\n",
            "Epoch: 183, Training Loss: 0.006259, Time: 21.2916 seconds\n",
            "Epoch: 184, Training Loss: 0.007007, Time: 20.4153 seconds\n",
            "Epoch: 185, Training Loss: 0.007981, Time: 21.1775 seconds\n",
            "Epoch: 186, Training Loss: 0.007249, Time: 20.7087 seconds\n",
            "Epoch: 187, Training Loss: 0.006882, Time: 21.3528 seconds\n",
            "Epoch: 188, Training Loss: 0.007883, Time: 20.9645 seconds\n",
            "Epoch: 189, Training Loss: 0.007256, Time: 22.1276 seconds\n",
            "Epoch: 190, Training Loss: 0.007076, Time: 20.837 seconds\n",
            "Epoch: 191, Training Loss: 0.006384, Time: 21.3396 seconds\n",
            "Epoch: 192, Training Loss: 0.007237, Time: 21.0409 seconds\n",
            "Epoch: 193, Training Loss: 0.008029, Time: 21.3526 seconds\n",
            "Epoch: 194, Training Loss: 0.006031, Time: 21.3937 seconds\n",
            "Epoch: 195, Training Loss: 0.005754, Time: 21.1641 seconds\n",
            "Epoch: 196, Training Loss: 0.006156, Time: 20.8996 seconds\n",
            "Epoch: 197, Training Loss: 0.005990, Time: 20.8052 seconds\n",
            "Epoch: 198, Training Loss: 0.005282, Time: 21.822 seconds\n",
            "Epoch: 199, Training Loss: 0.006326, Time: 21.2048 seconds\n",
            "Epoch: 200, Training Loss: 0.005956, Time: 21.1448 seconds\n",
            "Epoch: 201, Training Loss: 0.006231, Time: 21.0964 seconds\n",
            "Epoch: 202, Training Loss: 0.005685, Time: 20.8337 seconds\n",
            "Epoch: 203, Training Loss: 0.005449, Time: 21.5329 seconds\n",
            "Epoch: 204, Training Loss: 0.005510, Time: 21.428 seconds\n",
            "Epoch: 205, Training Loss: 0.005225, Time: 21.5618 seconds\n",
            "Epoch: 206, Training Loss: 0.005488, Time: 21.0601 seconds\n",
            "Epoch: 207, Training Loss: 0.005234, Time: 22.015 seconds\n",
            "Epoch: 208, Training Loss: 0.004642, Time: 21.1777 seconds\n",
            "Epoch: 209, Training Loss: 0.006230, Time: 21.7542 seconds\n",
            "Epoch: 210, Training Loss: 0.005131, Time: 21.4947 seconds\n",
            "Epoch: 211, Training Loss: 0.006486, Time: 21.5381 seconds\n",
            "Epoch: 212, Training Loss: 0.005473, Time: 20.8751 seconds\n",
            "Epoch: 213, Training Loss: 0.005341, Time: 21.4247 seconds\n",
            "Epoch: 214, Training Loss: 0.005200, Time: 20.9909 seconds\n",
            "Epoch: 215, Training Loss: 0.004763, Time: 21.2584 seconds\n",
            "Epoch: 216, Training Loss: 0.005272, Time: 22.1966 seconds\n",
            "Epoch: 217, Training Loss: 0.007002, Time: 21.5098 seconds\n",
            "Epoch: 218, Training Loss: 0.004611, Time: 21.2365 seconds\n",
            "Epoch: 219, Training Loss: 0.004897, Time: 21.3172 seconds\n",
            "Epoch: 220, Training Loss: 0.005032, Time: 20.9209 seconds\n",
            "Epoch: 221, Training Loss: 0.005573, Time: 21.9589 seconds\n",
            "Epoch: 222, Training Loss: 0.004381, Time: 21.6353 seconds\n",
            "Epoch: 223, Training Loss: 0.004560, Time: 21.3051 seconds\n",
            "Epoch: 224, Training Loss: 0.005512, Time: 21.0439 seconds\n",
            "Epoch: 225, Training Loss: 0.004592, Time: 21.364 seconds\n",
            "Epoch: 226, Training Loss: 0.005825, Time: 22.0843 seconds\n",
            "Epoch: 227, Training Loss: 0.004488, Time: 21.3499 seconds\n",
            "Epoch: 228, Training Loss: 0.004094, Time: 21.1703 seconds\n",
            "Epoch: 229, Training Loss: 0.004051, Time: 21.2302 seconds\n",
            "Epoch: 230, Training Loss: 0.004396, Time: 20.8858 seconds\n",
            "Epoch: 231, Training Loss: 0.005116, Time: 21.2934 seconds\n",
            "Epoch: 232, Training Loss: 0.004590, Time: 20.9596 seconds\n",
            "Epoch: 233, Training Loss: 0.004477, Time: 21.0276 seconds\n",
            "Epoch: 234, Training Loss: 0.004648, Time: 21.0568 seconds\n",
            "Epoch: 235, Training Loss: 0.003970, Time: 21.4658 seconds\n",
            "Epoch: 236, Training Loss: 0.003982, Time: 20.7493 seconds\n",
            "Epoch: 237, Training Loss: 0.003445, Time: 20.5487 seconds\n",
            "Epoch: 238, Training Loss: 0.004621, Time: 21.9576 seconds\n",
            "Epoch: 239, Training Loss: 0.003944, Time: 22.2369 seconds\n",
            "Epoch: 240, Training Loss: 0.003464, Time: 20.6781 seconds\n",
            "Epoch: 241, Training Loss: 0.004473, Time: 20.2146 seconds\n",
            "Epoch: 242, Training Loss: 0.004440, Time: 20.5561 seconds\n",
            "Epoch: 243, Training Loss: 0.003804, Time: 20.8689 seconds\n",
            "Epoch: 244, Training Loss: 0.004082, Time: 21.0822 seconds\n",
            "Epoch: 245, Training Loss: 0.004217, Time: 21.3377 seconds\n",
            "Epoch: 246, Training Loss: 0.003169, Time: 20.8894 seconds\n",
            "Epoch: 247, Training Loss: 0.003326, Time: 20.2025 seconds\n",
            "Epoch: 248, Training Loss: 0.003751, Time: 20.8225 seconds\n",
            "Epoch: 249, Training Loss: 0.003872, Time: 20.8703 seconds\n",
            "Epoch: 250, Training Loss: 0.004206, Time: 21.0079 seconds\n",
            "Epoch: 251, Training Loss: 0.004469, Time: 20.4517 seconds\n",
            "Epoch: 252, Training Loss: 0.003389, Time: 20.7078 seconds\n",
            "Epoch: 253, Training Loss: 0.003550, Time: 20.7637 seconds\n",
            "Epoch: 254, Training Loss: 0.003786, Time: 20.5275 seconds\n",
            "Epoch: 255, Training Loss: 0.003582, Time: 21.5766 seconds\n",
            "Epoch: 256, Training Loss: 0.002911, Time: 21.0017 seconds\n",
            "Epoch: 257, Training Loss: 0.003765, Time: 20.5748 seconds\n",
            "Epoch: 258, Training Loss: 0.003573, Time: 20.4594 seconds\n",
            "Epoch: 259, Training Loss: 0.003261, Time: 20.9464 seconds\n",
            "Epoch: 260, Training Loss: 0.003316, Time: 21.1951 seconds\n",
            "Epoch: 261, Training Loss: 0.003396, Time: 20.5287 seconds\n",
            "Epoch: 262, Training Loss: 0.003148, Time: 20.5618 seconds\n",
            "Epoch: 263, Training Loss: 0.003169, Time: 20.8184 seconds\n",
            "Epoch: 264, Training Loss: 0.003295, Time: 20.5306 seconds\n",
            "Epoch: 265, Training Loss: 0.002905, Time: 21.3482 seconds\n",
            "Epoch: 266, Training Loss: 0.002906, Time: 20.6614 seconds\n",
            "Epoch: 267, Training Loss: 0.002815, Time: 20.9599 seconds\n",
            "Epoch: 268, Training Loss: 0.003406, Time: 20.8946 seconds\n",
            "Epoch: 269, Training Loss: 0.002583, Time: 20.9434 seconds\n",
            "Epoch: 270, Training Loss: 0.003279, Time: 20.7132 seconds\n",
            "Epoch: 271, Training Loss: 0.003257, Time: 20.8004 seconds\n",
            "Epoch: 272, Training Loss: 0.002828, Time: 20.5402 seconds\n",
            "Epoch: 273, Training Loss: 0.002807, Time: 21.1134 seconds\n",
            "Epoch: 274, Training Loss: 0.003181, Time: 21.3543 seconds\n",
            "Epoch: 275, Training Loss: 0.002831, Time: 21.4677 seconds\n",
            "Epoch: 276, Training Loss: 0.002817, Time: 21.0072 seconds\n",
            "Epoch: 277, Training Loss: 0.002925, Time: 20.8898 seconds\n",
            "Epoch: 278, Training Loss: 0.003037, Time: 20.4934 seconds\n",
            "Epoch: 279, Training Loss: 0.002569, Time: 21.286 seconds\n",
            "Epoch: 280, Training Loss: 0.003309, Time: 20.4693 seconds\n",
            "Epoch: 281, Training Loss: 0.002677, Time: 21.035 seconds\n",
            "Epoch: 282, Training Loss: 0.002957, Time: 20.8686 seconds\n",
            "Epoch: 283, Training Loss: 0.003027, Time: 20.7918 seconds\n",
            "Epoch: 284, Training Loss: 0.003193, Time: 21.9645 seconds\n",
            "Epoch: 285, Training Loss: 0.002955, Time: 20.5791 seconds\n",
            "Epoch: 286, Training Loss: 0.003210, Time: 20.5816 seconds\n",
            "Epoch: 287, Training Loss: 0.002720, Time: 20.7664 seconds\n",
            "Epoch: 288, Training Loss: 0.002634, Time: 20.2347 seconds\n",
            "Epoch: 289, Training Loss: 0.002628, Time: 21.2378 seconds\n",
            "Epoch: 290, Training Loss: 0.003001, Time: 20.2598 seconds\n",
            "Epoch: 291, Training Loss: 0.002460, Time: 20.8107 seconds\n",
            "Epoch: 292, Training Loss: 0.002272, Time: 20.4744 seconds\n",
            "Epoch: 293, Training Loss: 0.002755, Time: 20.5775 seconds\n",
            "Epoch: 294, Training Loss: 0.002260, Time: 21.6275 seconds\n",
            "Epoch: 295, Training Loss: 0.002349, Time: 21.2062 seconds\n",
            "Epoch: 296, Training Loss: 0.002864, Time: 21.7329 seconds\n",
            "Epoch: 297, Training Loss: 0.002727, Time: 21.6533 seconds\n",
            "Epoch: 298, Training Loss: 0.002636, Time: 21.9964 seconds\n",
            "Epoch: 299, Training Loss: 0.002514, Time: 20.9366 seconds\n",
            "Epoch: 300, Training Loss: 0.002722, Time: 21.0351 seconds\n",
            "Total Time: 6340.0617 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "t5 = time.process_time()\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        batch_size = imgs.shape[0]\n",
        "        outputs = model2(imgs.view(batch_size, -1))\n",
        "        _, predicted = torch.max(outputs, dim = 1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "t6= time.process_time()\n",
        "print(f\"Accuracy: {round(correct/total, 3)}, Time: {round(t6 - t5, 3)} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5Fzk2ZllCQ-",
        "outputId": "2dcaa66c-93c4-49c6-a7a4-72f27a5a6123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.448, Time: 4.444 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part I Q2"
      ],
      "metadata": {
        "id": "TToRfbLTIoFD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xsG9dqhjIqSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "print(f\"{device} Training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJi5khg7l3Du",
        "outputId": "4333f8f0-77f9-4fa0-e6dc-2841c1adcfda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size = 3, padding = 1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size = 3, padding = 1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "SKjhtUMymX-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    tot5 = time.process_time()\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        t7 = time.process_time()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device = device)\n",
        "            labels = labels.to(device = device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        t8 = time.process_time()\n",
        "        print(f\"Epoch: %d, Training Loss: %f, Time: {round(t8-t7,4)}\" % (epoch, float(loss)))\n",
        "    tot6 = time.process_time()\n",
        "    print(f\"Total Time: {round(tot6-tot5,4)} seconds\")"
      ],
      "metadata": {
        "id": "6f2blu4OpWhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        t9 = time.process_time()\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device = device)\n",
        "                labels = labels.to(device = device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "        t10 = time.process_time()\n",
        "            \n",
        "        print(f\"Type: {name}, Accuracy: {round(correct/total, 3)}, Time: {round(t10-t9, 4)} seconds\")"
      ],
      "metadata": {
        "id": "Ws-4I0h84rMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelCNN = Net()\n",
        "if torch.cuda.is_available():\n",
        "  modelCNN.cuda()\n",
        "optimizerCNN = optim.SGD(modelCNN.parameters(), lr = learning_rate)\n",
        "loss_fnCNN = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(n_epochs,optimizerCNN, modelCNN, loss_fnCNN, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um5SCGp86D_N",
        "outputId": "c755edbf-0321-4b39-a101-5507e078e3b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 2.310158, Time: 36.9677\n",
            "Epoch: 2, Training Loss: 2.022371, Time: 37.5874\n",
            "Epoch: 3, Training Loss: 2.018423, Time: 37.7519\n",
            "Epoch: 4, Training Loss: 2.076935, Time: 37.9092\n",
            "Epoch: 5, Training Loss: 1.787677, Time: 37.2828\n",
            "Epoch: 6, Training Loss: 1.883313, Time: 37.7134\n",
            "Epoch: 7, Training Loss: 1.989109, Time: 36.6564\n",
            "Epoch: 8, Training Loss: 2.009430, Time: 37.0749\n",
            "Epoch: 9, Training Loss: 1.947320, Time: 37.3978\n",
            "Epoch: 10, Training Loss: 1.815207, Time: 36.6094\n",
            "Epoch: 11, Training Loss: 1.904891, Time: 36.6376\n",
            "Epoch: 12, Training Loss: 2.303844, Time: 36.9523\n",
            "Epoch: 13, Training Loss: 1.849602, Time: 36.6327\n",
            "Epoch: 14, Training Loss: 2.023801, Time: 38.1988\n",
            "Epoch: 15, Training Loss: 1.658250, Time: 36.5738\n",
            "Epoch: 16, Training Loss: 1.818741, Time: 36.207\n",
            "Epoch: 17, Training Loss: 1.344258, Time: 36.5084\n",
            "Epoch: 18, Training Loss: 1.483084, Time: 37.9684\n",
            "Epoch: 19, Training Loss: 1.426981, Time: 37.4061\n",
            "Epoch: 20, Training Loss: 1.418269, Time: 37.5173\n",
            "Epoch: 21, Training Loss: 1.535511, Time: 38.0969\n",
            "Epoch: 22, Training Loss: 1.482570, Time: 37.3715\n",
            "Epoch: 23, Training Loss: 1.477611, Time: 36.7693\n",
            "Epoch: 24, Training Loss: 1.771934, Time: 36.6733\n",
            "Epoch: 25, Training Loss: 1.486405, Time: 36.9647\n",
            "Epoch: 26, Training Loss: 1.761817, Time: 36.4301\n",
            "Epoch: 27, Training Loss: 1.529351, Time: 36.4719\n",
            "Epoch: 28, Training Loss: 1.563498, Time: 36.9352\n",
            "Epoch: 29, Training Loss: 1.491657, Time: 38.0651\n",
            "Epoch: 30, Training Loss: 1.999376, Time: 37.6162\n",
            "Epoch: 31, Training Loss: 1.608382, Time: 37.7459\n",
            "Epoch: 32, Training Loss: 1.401502, Time: 37.9752\n",
            "Epoch: 33, Training Loss: 1.265228, Time: 36.4516\n",
            "Epoch: 34, Training Loss: 2.026670, Time: 37.5812\n",
            "Epoch: 35, Training Loss: 1.729160, Time: 37.1773\n",
            "Epoch: 36, Training Loss: 1.186738, Time: 38.4601\n",
            "Epoch: 37, Training Loss: 1.491037, Time: 38.2076\n",
            "Epoch: 38, Training Loss: 1.677588, Time: 37.9806\n",
            "Epoch: 39, Training Loss: 1.555103, Time: 38.3113\n",
            "Epoch: 40, Training Loss: 1.554927, Time: 38.1299\n",
            "Epoch: 41, Training Loss: 1.692038, Time: 38.9936\n",
            "Epoch: 42, Training Loss: 1.352059, Time: 37.7454\n",
            "Epoch: 43, Training Loss: 1.497703, Time: 38.3265\n",
            "Epoch: 44, Training Loss: 1.071046, Time: 38.5375\n",
            "Epoch: 45, Training Loss: 1.065249, Time: 38.2077\n",
            "Epoch: 46, Training Loss: 1.500422, Time: 38.8213\n",
            "Epoch: 47, Training Loss: 1.150338, Time: 37.2249\n",
            "Epoch: 48, Training Loss: 1.372136, Time: 36.8553\n",
            "Epoch: 49, Training Loss: 1.713391, Time: 37.2927\n",
            "Epoch: 50, Training Loss: 1.482794, Time: 37.5941\n",
            "Epoch: 51, Training Loss: 1.006416, Time: 38.3697\n",
            "Epoch: 52, Training Loss: 1.335171, Time: 36.9244\n",
            "Epoch: 53, Training Loss: 1.534962, Time: 37.3621\n",
            "Epoch: 54, Training Loss: 1.200770, Time: 37.1441\n",
            "Epoch: 55, Training Loss: 1.917104, Time: 38.2419\n",
            "Epoch: 56, Training Loss: 1.689107, Time: 38.246\n",
            "Epoch: 57, Training Loss: 1.283197, Time: 36.9741\n",
            "Epoch: 58, Training Loss: 1.445598, Time: 39.2091\n",
            "Epoch: 59, Training Loss: 1.311220, Time: 38.1906\n",
            "Epoch: 60, Training Loss: 1.397670, Time: 37.2196\n",
            "Epoch: 61, Training Loss: 1.033861, Time: 37.4398\n",
            "Epoch: 62, Training Loss: 1.288671, Time: 37.6345\n",
            "Epoch: 63, Training Loss: 1.566881, Time: 37.6728\n",
            "Epoch: 64, Training Loss: 0.941497, Time: 38.4879\n",
            "Epoch: 65, Training Loss: 1.393374, Time: 37.4055\n",
            "Epoch: 66, Training Loss: 1.132418, Time: 37.6694\n",
            "Epoch: 67, Training Loss: 1.126385, Time: 37.2093\n",
            "Epoch: 68, Training Loss: 1.150807, Time: 36.2258\n",
            "Epoch: 69, Training Loss: 1.641649, Time: 37.0763\n",
            "Epoch: 70, Training Loss: 1.402231, Time: 36.1891\n",
            "Epoch: 71, Training Loss: 1.501915, Time: 36.9804\n",
            "Epoch: 72, Training Loss: 1.503446, Time: 36.9704\n",
            "Epoch: 73, Training Loss: 1.250345, Time: 37.2561\n",
            "Epoch: 74, Training Loss: 1.470720, Time: 36.4215\n",
            "Epoch: 75, Training Loss: 1.429067, Time: 37.1555\n",
            "Epoch: 76, Training Loss: 1.087183, Time: 36.5572\n",
            "Epoch: 77, Training Loss: 0.872176, Time: 36.1908\n",
            "Epoch: 78, Training Loss: 1.220228, Time: 36.295\n",
            "Epoch: 79, Training Loss: 1.265134, Time: 37.5023\n",
            "Epoch: 80, Training Loss: 1.761062, Time: 36.4225\n",
            "Epoch: 81, Training Loss: 1.278699, Time: 36.3302\n",
            "Epoch: 82, Training Loss: 1.634974, Time: 36.9686\n",
            "Epoch: 83, Training Loss: 1.332469, Time: 36.6347\n",
            "Epoch: 84, Training Loss: 0.918327, Time: 37.0651\n",
            "Epoch: 85, Training Loss: 0.959048, Time: 36.8327\n",
            "Epoch: 86, Training Loss: 1.297974, Time: 37.0925\n",
            "Epoch: 87, Training Loss: 1.181456, Time: 37.7719\n",
            "Epoch: 88, Training Loss: 0.951811, Time: 37.8907\n",
            "Epoch: 89, Training Loss: 1.200282, Time: 39.0203\n",
            "Epoch: 90, Training Loss: 1.065073, Time: 37.9088\n",
            "Epoch: 91, Training Loss: 1.331486, Time: 37.1437\n",
            "Epoch: 92, Training Loss: 0.850644, Time: 39.1101\n",
            "Epoch: 93, Training Loss: 1.396236, Time: 37.6349\n",
            "Epoch: 94, Training Loss: 1.347390, Time: 37.0024\n",
            "Epoch: 95, Training Loss: 0.896225, Time: 37.0731\n",
            "Epoch: 96, Training Loss: 1.068021, Time: 38.4679\n",
            "Epoch: 97, Training Loss: 1.158387, Time: 37.9136\n",
            "Epoch: 98, Training Loss: 1.368033, Time: 36.3392\n",
            "Epoch: 99, Training Loss: 1.379148, Time: 37.3978\n",
            "Epoch: 100, Training Loss: 1.396146, Time: 36.9579\n",
            "Epoch: 101, Training Loss: 1.286302, Time: 36.4594\n",
            "Epoch: 102, Training Loss: 0.866670, Time: 37.418\n",
            "Epoch: 103, Training Loss: 0.984463, Time: 36.8972\n",
            "Epoch: 104, Training Loss: 1.080664, Time: 36.2529\n",
            "Epoch: 105, Training Loss: 1.292523, Time: 36.5828\n",
            "Epoch: 106, Training Loss: 0.818588, Time: 36.3495\n",
            "Epoch: 107, Training Loss: 1.380585, Time: 37.3966\n",
            "Epoch: 108, Training Loss: 1.228771, Time: 36.8345\n",
            "Epoch: 109, Training Loss: 1.333920, Time: 37.8076\n",
            "Epoch: 110, Training Loss: 0.952544, Time: 36.7661\n",
            "Epoch: 111, Training Loss: 0.889687, Time: 36.9365\n",
            "Epoch: 112, Training Loss: 0.920746, Time: 36.9442\n",
            "Epoch: 113, Training Loss: 0.922351, Time: 37.5588\n",
            "Epoch: 114, Training Loss: 1.537163, Time: 37.2303\n",
            "Epoch: 115, Training Loss: 0.979526, Time: 38.0493\n",
            "Epoch: 116, Training Loss: 1.605279, Time: 36.6942\n",
            "Epoch: 117, Training Loss: 1.268182, Time: 36.8382\n",
            "Epoch: 118, Training Loss: 1.612008, Time: 36.712\n",
            "Epoch: 119, Training Loss: 1.482031, Time: 36.5266\n",
            "Epoch: 120, Training Loss: 1.394706, Time: 37.2866\n",
            "Epoch: 121, Training Loss: 0.961401, Time: 36.5484\n",
            "Epoch: 122, Training Loss: 1.184294, Time: 37.1386\n",
            "Epoch: 123, Training Loss: 1.609162, Time: 36.762\n",
            "Epoch: 124, Training Loss: 1.215950, Time: 36.5338\n",
            "Epoch: 125, Training Loss: 1.024540, Time: 36.2726\n",
            "Epoch: 126, Training Loss: 0.881637, Time: 36.5803\n",
            "Epoch: 127, Training Loss: 0.832082, Time: 37.1981\n",
            "Epoch: 128, Training Loss: 1.050384, Time: 36.6629\n",
            "Epoch: 129, Training Loss: 1.528978, Time: 37.0138\n",
            "Epoch: 130, Training Loss: 1.262655, Time: 36.7973\n",
            "Epoch: 131, Training Loss: 1.034978, Time: 36.7553\n",
            "Epoch: 132, Training Loss: 0.732615, Time: 37.0096\n",
            "Epoch: 133, Training Loss: 1.469232, Time: 37.5054\n",
            "Epoch: 134, Training Loss: 0.848848, Time: 36.631\n",
            "Epoch: 135, Training Loss: 1.209677, Time: 37.1908\n",
            "Epoch: 136, Training Loss: 1.466240, Time: 36.5896\n",
            "Epoch: 137, Training Loss: 1.596304, Time: 36.1954\n",
            "Epoch: 138, Training Loss: 1.281484, Time: 36.5134\n",
            "Epoch: 139, Training Loss: 0.844823, Time: 36.4615\n",
            "Epoch: 140, Training Loss: 1.101980, Time: 37.6151\n",
            "Epoch: 141, Training Loss: 1.004859, Time: 36.5758\n",
            "Epoch: 142, Training Loss: 1.135076, Time: 36.8578\n",
            "Epoch: 143, Training Loss: 0.976896, Time: 36.8786\n",
            "Epoch: 144, Training Loss: 1.337299, Time: 36.9981\n",
            "Epoch: 145, Training Loss: 1.099359, Time: 37.1818\n",
            "Epoch: 146, Training Loss: 1.099592, Time: 37.8026\n",
            "Epoch: 147, Training Loss: 0.897255, Time: 37.2798\n",
            "Epoch: 148, Training Loss: 0.814994, Time: 37.3461\n",
            "Epoch: 149, Training Loss: 1.692898, Time: 36.9096\n",
            "Epoch: 150, Training Loss: 1.259561, Time: 37.0263\n",
            "Epoch: 151, Training Loss: 0.826756, Time: 36.4706\n",
            "Epoch: 152, Training Loss: 1.226991, Time: 36.3139\n",
            "Epoch: 153, Training Loss: 1.228776, Time: 37.0584\n",
            "Epoch: 154, Training Loss: 0.919747, Time: 36.3894\n",
            "Epoch: 155, Training Loss: 1.019251, Time: 36.9531\n",
            "Epoch: 156, Training Loss: 0.748802, Time: 36.5085\n",
            "Epoch: 157, Training Loss: 1.103072, Time: 36.6137\n",
            "Epoch: 158, Training Loss: 0.835474, Time: 36.3877\n",
            "Epoch: 159, Training Loss: 0.690520, Time: 36.6456\n",
            "Epoch: 160, Training Loss: 1.057338, Time: 37.0896\n",
            "Epoch: 161, Training Loss: 1.310537, Time: 37.0354\n",
            "Epoch: 162, Training Loss: 0.683963, Time: 36.4261\n",
            "Epoch: 163, Training Loss: 1.156834, Time: 36.2691\n",
            "Epoch: 164, Training Loss: 1.149043, Time: 36.8586\n",
            "Epoch: 165, Training Loss: 1.218973, Time: 37.1199\n",
            "Epoch: 166, Training Loss: 1.649097, Time: 37.1956\n",
            "Epoch: 167, Training Loss: 0.997059, Time: 36.4175\n",
            "Epoch: 168, Training Loss: 1.036528, Time: 37.1335\n",
            "Epoch: 169, Training Loss: 0.835540, Time: 36.6555\n",
            "Epoch: 170, Training Loss: 0.724260, Time: 36.475\n",
            "Epoch: 171, Training Loss: 1.026475, Time: 36.1961\n",
            "Epoch: 172, Training Loss: 1.074252, Time: 36.4902\n",
            "Epoch: 173, Training Loss: 1.212367, Time: 36.7286\n",
            "Epoch: 174, Training Loss: 1.039616, Time: 37.4948\n",
            "Epoch: 175, Training Loss: 0.881666, Time: 36.6578\n",
            "Epoch: 176, Training Loss: 0.904297, Time: 36.2945\n",
            "Epoch: 177, Training Loss: 1.349935, Time: 36.3242\n",
            "Epoch: 178, Training Loss: 1.211310, Time: 36.5408\n",
            "Epoch: 179, Training Loss: 1.267588, Time: 37.8382\n",
            "Epoch: 180, Training Loss: 1.093966, Time: 36.2612\n",
            "Epoch: 181, Training Loss: 0.657609, Time: 37.0797\n",
            "Epoch: 182, Training Loss: 0.866339, Time: 36.1821\n",
            "Epoch: 183, Training Loss: 1.201347, Time: 36.4791\n",
            "Epoch: 184, Training Loss: 0.769449, Time: 36.2748\n",
            "Epoch: 185, Training Loss: 0.941848, Time: 36.0712\n",
            "Epoch: 186, Training Loss: 1.250519, Time: 37.0317\n",
            "Epoch: 187, Training Loss: 0.922766, Time: 37.5654\n",
            "Epoch: 188, Training Loss: 0.925990, Time: 36.8277\n",
            "Epoch: 189, Training Loss: 1.030101, Time: 37.1508\n",
            "Epoch: 190, Training Loss: 0.785976, Time: 37.1387\n",
            "Epoch: 191, Training Loss: 0.693236, Time: 36.5241\n",
            "Epoch: 192, Training Loss: 0.961986, Time: 37.8638\n",
            "Epoch: 193, Training Loss: 0.572195, Time: 38.0369\n",
            "Epoch: 194, Training Loss: 1.160947, Time: 36.9873\n",
            "Epoch: 195, Training Loss: 0.544569, Time: 37.126\n",
            "Epoch: 196, Training Loss: 1.057338, Time: 36.7112\n",
            "Epoch: 197, Training Loss: 1.271935, Time: 36.6143\n",
            "Epoch: 198, Training Loss: 1.093159, Time: 36.4348\n",
            "Epoch: 199, Training Loss: 0.906495, Time: 37.5881\n",
            "Epoch: 200, Training Loss: 0.648454, Time: 37.6005\n",
            "Epoch: 201, Training Loss: 0.802071, Time: 37.2844\n",
            "Epoch: 202, Training Loss: 1.260888, Time: 36.6945\n",
            "Epoch: 203, Training Loss: 0.801291, Time: 36.6613\n",
            "Epoch: 204, Training Loss: 1.058769, Time: 37.3191\n",
            "Epoch: 205, Training Loss: 1.579609, Time: 36.7788\n",
            "Epoch: 206, Training Loss: 0.980258, Time: 36.8269\n",
            "Epoch: 207, Training Loss: 1.065502, Time: 36.8516\n",
            "Epoch: 208, Training Loss: 1.100578, Time: 36.0293\n",
            "Epoch: 209, Training Loss: 1.036543, Time: 36.9372\n",
            "Epoch: 210, Training Loss: 0.958947, Time: 36.4136\n",
            "Epoch: 211, Training Loss: 1.235759, Time: 37.1743\n",
            "Epoch: 212, Training Loss: 1.096920, Time: 37.0447\n",
            "Epoch: 213, Training Loss: 0.655817, Time: 36.2249\n",
            "Epoch: 214, Training Loss: 0.882368, Time: 36.4832\n",
            "Epoch: 215, Training Loss: 1.307241, Time: 36.1975\n",
            "Epoch: 216, Training Loss: 0.836425, Time: 36.5154\n",
            "Epoch: 217, Training Loss: 0.905210, Time: 36.9173\n",
            "Epoch: 218, Training Loss: 0.351058, Time: 37.3491\n",
            "Epoch: 219, Training Loss: 0.734315, Time: 37.5613\n",
            "Epoch: 220, Training Loss: 0.817751, Time: 36.6991\n",
            "Epoch: 221, Training Loss: 0.892263, Time: 36.9493\n",
            "Epoch: 222, Training Loss: 0.665039, Time: 36.5244\n",
            "Epoch: 223, Training Loss: 0.838374, Time: 36.7369\n",
            "Epoch: 224, Training Loss: 0.734923, Time: 37.4925\n",
            "Epoch: 225, Training Loss: 1.507796, Time: 37.1491\n",
            "Epoch: 226, Training Loss: 0.912363, Time: 36.3459\n",
            "Epoch: 227, Training Loss: 1.189770, Time: 36.6082\n",
            "Epoch: 228, Training Loss: 1.494779, Time: 36.5589\n",
            "Epoch: 229, Training Loss: 0.606250, Time: 36.5079\n",
            "Epoch: 230, Training Loss: 1.109653, Time: 37.1139\n",
            "Epoch: 231, Training Loss: 0.975065, Time: 37.1785\n",
            "Epoch: 232, Training Loss: 0.935949, Time: 36.3084\n",
            "Epoch: 233, Training Loss: 0.758746, Time: 36.5187\n",
            "Epoch: 234, Training Loss: 1.229466, Time: 36.4658\n",
            "Epoch: 235, Training Loss: 1.388126, Time: 36.3156\n",
            "Epoch: 236, Training Loss: 0.713910, Time: 37.1021\n",
            "Epoch: 237, Training Loss: 1.211261, Time: 36.9463\n",
            "Epoch: 238, Training Loss: 1.145085, Time: 36.525\n",
            "Epoch: 239, Training Loss: 0.664775, Time: 36.3302\n",
            "Epoch: 240, Training Loss: 0.596792, Time: 36.5456\n",
            "Epoch: 241, Training Loss: 0.648315, Time: 36.6223\n",
            "Epoch: 242, Training Loss: 0.490985, Time: 36.4811\n",
            "Epoch: 243, Training Loss: 0.882628, Time: 37.3251\n",
            "Epoch: 244, Training Loss: 1.071842, Time: 37.0667\n",
            "Epoch: 245, Training Loss: 1.285208, Time: 37.418\n",
            "Epoch: 246, Training Loss: 0.778121, Time: 36.6195\n",
            "Epoch: 247, Training Loss: 0.631133, Time: 36.4956\n",
            "Epoch: 248, Training Loss: 0.784530, Time: 36.5372\n",
            "Epoch: 249, Training Loss: 1.082843, Time: 36.5563\n",
            "Epoch: 250, Training Loss: 0.925639, Time: 37.6994\n",
            "Epoch: 251, Training Loss: 1.476950, Time: 36.2023\n",
            "Epoch: 252, Training Loss: 1.277070, Time: 36.7052\n",
            "Epoch: 253, Training Loss: 1.473878, Time: 36.8244\n",
            "Epoch: 254, Training Loss: 1.175147, Time: 36.3982\n",
            "Epoch: 255, Training Loss: 0.844797, Time: 36.2038\n",
            "Epoch: 256, Training Loss: 1.172255, Time: 37.9532\n",
            "Epoch: 257, Training Loss: 0.767412, Time: 36.3007\n",
            "Epoch: 258, Training Loss: 0.332761, Time: 36.2853\n",
            "Epoch: 259, Training Loss: 0.986677, Time: 36.5089\n",
            "Epoch: 260, Training Loss: 0.929624, Time: 36.4186\n",
            "Epoch: 261, Training Loss: 0.551498, Time: 35.9904\n",
            "Epoch: 262, Training Loss: 1.419072, Time: 37.5912\n",
            "Epoch: 263, Training Loss: 0.802588, Time: 36.7994\n",
            "Epoch: 264, Training Loss: 1.019210, Time: 36.4587\n",
            "Epoch: 265, Training Loss: 1.199285, Time: 36.3422\n",
            "Epoch: 266, Training Loss: 0.673105, Time: 36.3502\n",
            "Epoch: 267, Training Loss: 0.992191, Time: 36.277\n",
            "Epoch: 268, Training Loss: 0.931925, Time: 36.4789\n",
            "Epoch: 269, Training Loss: 0.322352, Time: 37.6554\n",
            "Epoch: 270, Training Loss: 1.317044, Time: 36.4602\n",
            "Epoch: 271, Training Loss: 0.821407, Time: 36.5797\n",
            "Epoch: 272, Training Loss: 0.819301, Time: 36.3603\n",
            "Epoch: 273, Training Loss: 0.942550, Time: 36.9143\n",
            "Epoch: 274, Training Loss: 0.957118, Time: 36.3218\n",
            "Epoch: 275, Training Loss: 0.691502, Time: 38.3909\n",
            "Epoch: 276, Training Loss: 1.276941, Time: 36.6498\n",
            "Epoch: 277, Training Loss: 0.726951, Time: 36.2937\n",
            "Epoch: 278, Training Loss: 0.485359, Time: 36.462\n",
            "Epoch: 279, Training Loss: 0.930059, Time: 37.3266\n",
            "Epoch: 280, Training Loss: 0.430414, Time: 36.7927\n",
            "Epoch: 281, Training Loss: 0.930745, Time: 36.3273\n",
            "Epoch: 282, Training Loss: 0.766626, Time: 36.6522\n",
            "Epoch: 283, Training Loss: 1.204407, Time: 38.1772\n",
            "Epoch: 284, Training Loss: 0.797567, Time: 36.501\n",
            "Epoch: 285, Training Loss: 0.966935, Time: 36.0984\n",
            "Epoch: 286, Training Loss: 0.885133, Time: 36.3967\n",
            "Epoch: 287, Training Loss: 0.792524, Time: 36.6545\n",
            "Epoch: 288, Training Loss: 1.030017, Time: 36.1902\n",
            "Epoch: 289, Training Loss: 1.027103, Time: 36.2663\n",
            "Epoch: 290, Training Loss: 1.155530, Time: 36.6184\n",
            "Epoch: 291, Training Loss: 0.763742, Time: 36.2548\n",
            "Epoch: 292, Training Loss: 0.916639, Time: 37.5461\n",
            "Epoch: 293, Training Loss: 0.861284, Time: 36.7797\n",
            "Epoch: 294, Training Loss: 0.969453, Time: 36.6858\n",
            "Epoch: 295, Training Loss: 0.557421, Time: 36.079\n",
            "Epoch: 296, Training Loss: 1.194994, Time: 36.5723\n",
            "Epoch: 297, Training Loss: 0.607201, Time: 37.0805\n",
            "Epoch: 298, Training Loss: 1.188198, Time: 36.5019\n",
            "Epoch: 299, Training Loss: 0.765123, Time: 36.8165\n",
            "Epoch: 300, Training Loss: 0.792117, Time: 37.2489\n",
            "Total Time: 11099.2155 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validate(modelCNN, train_loader, val_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpLPcHC6QGKu",
        "outputId": "24cbfda9-4f60-4b08-c764-78e5793df9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type: train, Accuracy: 0.684, Time: 30.26 seconds\n",
            "Type: val, Accuracy: 0.655, Time: 5.6859 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part II Q2"
      ],
      "metadata": {
        "id": "0CCkRoK9TdXo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0faHvqXeTfRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size = 3, padding = 1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(32, 18, kernel_size = 3, padding = 1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(18, 4, kernel_size = 3, padding = 1)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(4 * 4 * 4, 32)\n",
        "        self.act4 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 15)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = self.pool3(self.act3(self.conv3(out)))\n",
        "        out = out.view(-1, 4 * 4 * 4)\n",
        "        out = self.act4(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "kedTosrkTuh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    tot5 = time.process_time()\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        t7 = time.process_time()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device = device)\n",
        "            labels = labels.to(device = device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        t8 = time.process_time()\n",
        "        print(f\"Epoch: %d, Training Loss: %f, Time: {round(t8-t7,4)}\" % (epoch, float(loss)))\n",
        "    tot6 = time.process_time()\n",
        "    print(f\"Total Time: {round(tot6-tot5,4)} seconds\")"
      ],
      "metadata": {
        "id": "II_KVNFTTuh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        t9 = time.process_time()\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device = device)\n",
        "                labels = labels.to(device = device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "        t10 = time.process_time()\n",
        "            \n",
        "        print(f\"Type: {name}, Accuracy: {round(correct/total, 3)}, Time: {round(t10-t9, 4)} seconds\")"
      ],
      "metadata": {
        "id": "ZqIjIIDeTuh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelCNN2 = Net2()\n",
        "if torch.cuda.is_available():\n",
        "  modelCNN2.cuda()\n",
        "optimizerCNN2 = optim.SGD(modelCNN2.parameters(), lr = learning_rate)\n",
        "loss_fnCNN2 = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(n_epochs, optimizerCNN2, modelCNN2, loss_fnCNN2, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHTO9kvxHaWs",
        "outputId": "c2eb109e-e1ed-4426-d529-4e460eb8f229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 2.662158, Time: 13.6426\n",
            "Epoch: 2, Training Loss: 2.634050, Time: 11.8564\n",
            "Epoch: 3, Training Loss: 2.614000, Time: 12.9294\n",
            "Epoch: 4, Training Loss: 2.570090, Time: 11.697\n",
            "Epoch: 5, Training Loss: 2.540844, Time: 11.6525\n",
            "Epoch: 6, Training Loss: 2.525605, Time: 11.6581\n",
            "Epoch: 7, Training Loss: 2.492418, Time: 11.7993\n",
            "Epoch: 8, Training Loss: 2.452300, Time: 11.6765\n",
            "Epoch: 9, Training Loss: 2.429855, Time: 11.8749\n",
            "Epoch: 10, Training Loss: 2.393336, Time: 11.6631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validate(modelCNN2, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "p6A6-9K3IHDx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}